{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Khởi tạo Spark Session\n",
    "print(\"Initializing Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Real Estate Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.jars.packages\", \"ml.dmlc:xgboost4j-spark_2.12:1.5.2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ====================== PHẦN TIỀN XỬ LÝ DỮ LIỆU ======================\n",
    "# Đọc dữ liệu\n",
    "print(\"Đang đọc dữ liệu...\")\n",
    "df = spark.read.csv(\"realtor-data.zip.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Kiểm tra thông tin dữ liệu gốc\n",
    "df.printSchema()\n",
    "# Loại bỏ các cột không cần thiết\n",
    "df = df.drop(\"city\", \"zip_code\", \"prev_sold_date\")\n",
    "# Kiểm tra số lượng giá trị null\n",
    "null_counts = df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns])\n",
    "print(\"\\nSố lượng giá trị null:\")\n",
    "null_counts.show()\n",
    "\n",
    "\n",
    "\n",
    "# Loại bỏ các hàng có giá trị null trong cột quan trọng\n",
    "df = df.filter(~col(\"price\").isNull())\n",
    "df = df.filter(~col(\"bed\").isNull())\n",
    "df = df.filter(~col(\"bath\").isNull())\n",
    "\n",
    "# Xử lý cột status\n",
    "print(\"\\nGiá trị trong cột status:\")\n",
    "df.groupBy(\"status\").count().show()\n",
    "df = df.drop(\"status\")\n",
    "\n",
    "# Xử lý cột state\n",
    "print(\"\\nGiá trị trong cột state:\")\n",
    "df.groupBy(\"state\").count().show()\n",
    "\n",
    "# Chỉ giữ lại các state có ít nhất 50 mẫu\n",
    "state_counts = df.groupBy(\"state\").count()\n",
    "valid_states = state_counts.filter(col(\"count\") >= 50).select(\"state\")\n",
    "df = df.join(valid_states, \"state\", \"inner\")\n",
    "\n",
    "# Mã hóa state thành số\n",
    "indexer = StringIndexer(inputCol=\"state\", outputCol=\"state_numeric\", handleInvalid=\"skip\")\n",
    "indexer_model = indexer.fit(df)\n",
    "df = indexer_model.transform(df)\n",
    "df = df.drop(\"state\")\n",
    "\n",
    "# Tạo mapping từ số đến tên state\n",
    "state_labels = indexer_model.labels\n",
    "numeric_to_state = {i: state for i, state in enumerate(state_labels)}\n",
    "print(\"\\nMapping state_numeric -> state:\")\n",
    "for k, v in numeric_to_state.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Hiển thị thống kê mô tả\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý outlier cho cột price\n",
    "q95 = df.approxQuantile(\"price\", [0.95], 0.01)[0]\n",
    "q25 = df.approxQuantile(\"price\", [0.25], 0.01)[0]\n",
    "iqrMax = q95 + q25\n",
    "print(f\"\\nThreshold cho price: {iqrMax}\")\n",
    "percent_outliers = df.filter(col(\"price\") > 3150000.0).count() / df.count() * 100\n",
    "print(f\"Tỷ lệ outliers trong price: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"price\") <= 3150000.0)\n",
    "\n",
    "# Xử lý outlier cho acre_lot\n",
    "percent_outliers = df.filter(col(\"acre_lot\") > 200).count() / df.count() * 100\n",
    "print(f\"\\nTỷ lệ outliers trong acre_lot: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"acre_lot\") <= 200)\n",
    "\n",
    "# Xử lý outlier cho house_size\n",
    "percent_outliers = df.filter(col(\"house_size\") >= 20000).count() / df.count() * 100\n",
    "print(f\"\\nTỷ lệ outliers trong house_size: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"house_size\") < 20000)\n",
    "\n",
    "# Kiểm tra lại giá trị null sau khi xử lý outliers\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "print(\"\\nSố lượng giá trị null sau khi xử lý outliers:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Hiển thị thống kê mô tả\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id, when\n",
    "\n",
    "print(\"\\nĐang điền giá trị thiếu cho acre_lot bằng XGBoost...\")\n",
    "\n",
    "# Kiểm tra nếu có giá trị thiếu\n",
    "if df.filter(col(\"acre_lot\").isNull()).count() > 0:\n",
    "\n",
    "    # Chuyển dữ liệu có acre_lot sang Pandas\n",
    "    filled_pdf = filled_df.toPandas()\n",
    "    missing_pdf = missing_df.toPandas()\n",
    "\n",
    "    # Kiểm tra nếu có dữ liệu để huấn luyện\n",
    "    if not filled_pdf.empty and not missing_pdf.empty:\n",
    "        # Chuẩn bị dữ liệu huấn luyện\n",
    "        X_train_fill = filled_pdf.drop(columns=[\"acre_lot\"])\n",
    "        y_train_fill = filled_pdf[\"acre_lot\"]\n",
    "\n",
    "        X_test_fill = missing_pdf.drop(columns=[\"acre_lot\"])\n",
    "\n",
    "        # Huấn luyện mô hình XGBoost\n",
    "        model_fill = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8\n",
    "        )\n",
    "        model_fill.fit(X_train_fill, y_train_fill)\n",
    "\n",
    "        # Dự đoán giá trị thiếu\n",
    "        preds = model_fill.predict(X_test_fill)\n",
    "\n",
    "        # Chuyển kết quả về DataFrame PySpark\n",
    "        preds_df = pd.DataFrame({\"acre_lot\": preds})\n",
    "        preds_spark = spark.createDataFrame(preds_df)\n",
    "\n",
    "        # Thêm ID để kết hợp lại dữ liệu\n",
    "        df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        missing_df = missing_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds_spark = preds_spark.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "        # Kết hợp giá trị dự đoán vào tập dữ liệu gốc\n",
    "        df = df.join(preds_spark, \"id\", \"left_outer\").drop(\"id\")\n",
    "        df = df.withColumn(\"acre_lot\", when(col(\"acre_lot\").isNull(), col(\"prediction\")).otherwise(col(\"acre_lot\"))).drop(\"prediction\")\n",
    "\n",
    "        print(f\"Đã điền {len(preds)} giá trị thiếu cho acre_lot bằng XGBoost!\")\n",
    "\n",
    "    else:\n",
    "        print(\"Không có đủ dữ liệu để huấn luyện XGBoost. Bỏ qua việc điền giá trị thiếu.\")\n",
    "\n",
    "else:\n",
    "    print(\"Không có giá trị thiếu trong acre_lot. Không cần điền.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id, when\n",
    "\n",
    "print(\"\\nĐang điền giá trị thiếu cho house_size bằng XGBoost...\")\n",
    "\n",
    "# Kiểm tra nếu có giá trị thiếu\n",
    "if df.filter(col(\"house_size\").isNull()).count() > 0:\n",
    "\n",
    "    # Chuyển dữ liệu PySpark sang Pandas\n",
    "    filled_pdf = df.filter(col(\"house_size\").isNotNull()).toPandas()\n",
    "    missing_pdf = df.filter(col(\"house_size\").isNull()).toPandas()\n",
    "\n",
    "    # Kiểm tra nếu có đủ dữ liệu để huấn luyện\n",
    "    if not filled_pdf.empty and not missing_pdf.empty:\n",
    "        # Chuẩn bị dữ liệu huấn luyện\n",
    "        X_train_fill = filled_pdf.drop(columns=[\"house_size\"])\n",
    "        y_train_fill = filled_pdf[\"house_size\"]\n",
    "\n",
    "        X_test_fill = missing_pdf.drop(columns=[\"house_size\"])\n",
    "\n",
    "        # Xử lý giá trị NaN bằng giá trị trung bình\n",
    "        X_train_fill = X_train_fill.fillna(X_train_fill.mean())\n",
    "        X_test_fill = X_test_fill.fillna(X_train_fill.mean())\n",
    "\n",
    "        # Huấn luyện mô hình XGBoost\n",
    "        model_fill = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method='hist'\n",
    "        )\n",
    "        model_fill.fit(X_train_fill, y_train_fill)\n",
    "\n",
    "        # Dự đoán giá trị thiếu\n",
    "        preds = model_fill.predict(X_test_fill)\n",
    "\n",
    "        # Chuyển kết quả về DataFrame PySpark\n",
    "        preds_df = pd.DataFrame({\"house_size\": preds})\n",
    "        preds_spark = spark.createDataFrame(preds_df)\n",
    "\n",
    "        # Thêm ID để kết hợp lại dữ liệu\n",
    "        df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        missing_df = df.filter(col(\"house_size\").isNull()).withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds_spark = preds_spark.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "        # Kết hợp giá trị dự đoán vào tập dữ liệu gốc\n",
    "        df = df.join(preds_spark, \"id\", \"left_outer\").drop(\"id\")\n",
    "        df = df.withColumn(\"house_size\", when(col(\"house_size\").isNull(), col(\"prediction\")).otherwise(col(\"house_size\"))).drop(\"prediction\")\n",
    "\n",
    "        print(f\"Đã điền {len(preds)} giá trị thiếu cho house_size bằng XGBoost!\")\n",
    "\n",
    "    else:\n",
    "        print(\"Không có đủ dữ liệu để huấn luyện XGBoost. Bỏ qua việc điền giá trị thiếu.\")\n",
    "\n",
    "else:\n",
    "    print(\"Không có giá trị thiếu trong house_size. Không cần điền.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "print(\"Đang tạo các đặc trưng mới...\")\n",
    "\n",
    "# Tránh chia cho 0 bằng cách sử dụng F.when\n",
    "df = df.withColumn(\"bed_bath_ratio\", col(\"bed\") / when(col(\"bath\") == 0, 1).otherwise(col(\"bath\")))\n",
    "df = df.withColumn(\"total_rooms\", col(\"bed\") + col(\"bath\"))\n",
    "df = df.withColumn(\"room_density\", col(\"total_rooms\") / when(col(\"house_size\") == 0, 1).otherwise(col(\"house_size\")))\n",
    "df = df.withColumn(\"house_size_per_bed\", col(\"house_size\") / when(col(\"bed\") == 0, 1).otherwise(col(\"bed\")))\n",
    "df = df.withColumn(\"house_size_per_bath\", col(\"house_size\") / when(col(\"bath\") == 0, 1).otherwise(col(\"bath\")))\n",
    "df = df.withColumn(\"lot_to_house_ratio\", col(\"acre_lot\") / when(col(\"house_size\") == 0, 1).otherwise(col(\"house_size\")))\n",
    "df = df.withColumn(\"size_by_state\", col(\"house_size\") * col(\"state_numeric\"))\n",
    "df = df.withColumn(\"rooms_by_state\", col(\"total_rooms\") * col(\"state_numeric\"))\n",
    "\n",
    "# Điền giá trị thiếu cho các cột số (tránh lỗi)\n",
    "numeric_columns = [c for c, t in df.dtypes if t in ('int', 'double')]\n",
    "df = df.fillna(0, subset=numeric_columns)\n",
    "\n",
    "print(\"Hoàn tất tạo đặc trưng mới!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nChuẩn bị dữ liệu cho mô hình...\")\n",
    "\n",
    "# Xác định cột đặc trưng (bỏ cột price)\n",
    "feature_cols = [c for c in df.columns if c != \"price\"]\n",
    "\n",
    "# Dùng VectorAssembler để gộp tất cả đặc trưng vào một cột duy nhất\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"price\")\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và kiểm tra (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Kích thước tập huấn luyện: {train_df.count()} mẫu, tập kiểm tra: {test_df.count()} mẫu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nBắt đầu tìm kiếm siêu tham số tối ưu...\")\n",
    "\n",
    "# Sử dụng GBTRegressor với các tham số được tối ưu\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"price\",\n",
    "    maxBins=32,        # Tăng maxBins để cải thiện hiệu suất\n",
    "    maxIter=100,       # Số lần lặp\n",
    "    stepSize=0.1,      # Learning rate\n",
    "    maxDepth=5,        # Độ sâu cây\n",
    "    subsamplingRate=0.8  # Subsampling rate\n",
    ")\n",
    "\n",
    "# Giảm tham số trong lưới tìm kiếm để tránh timeout\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# Đánh giá mô hình\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Sử dụng TrainValidationSplit với tỷ lệ train cao hơn\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.9,  # 90% huấn luyện, 10% kiểm định\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "print(\"Đang huấn luyện mô hình, vui lòng đợi...\")\n",
    "model = tvs.fit(train_df)\n",
    "\n",
    "# Lấy mô hình tốt nhất\n",
    "best_model = model.bestModel\n",
    "print(\"\\nĐã tìm thấy mô hình tốt nhất!\")\n",
    "print(f\"Best maxDepth: {best_model.getMaxDepth()}\")\n",
    "print(f\"Best maxIter: {best_model.getMaxIter()}\")\n",
    "print(f\"Best stepSize: {best_model.getStepSize()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nĐánh giá mô hình trên tập kiểm tra...\")\n",
    "\n",
    "# Dự đoán\n",
    "predictions = best_model.transform(test_df)\n",
    "\n",
    "# Khởi tạo bộ đánh giá\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# Tính toán các chỉ số đánh giá\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(\"\\n🔍 Đánh giá mô hình trên dữ liệu thực tế:\")\n",
    "print(f\"✅ R² Score: {r2:.4f}\")\n",
    "print(f\"✅ RMSE: ${rmse:.2f}\")\n",
    "print(f\"✅ MAE: ${mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Đang vẽ biểu đồ...\")\n",
    "\n",
    "# Chuyển dữ liệu PySpark về Pandas\n",
    "predictions_pd = predictions.select(\"price\", \"prediction\").toPandas()\n",
    "\n",
    "# 1. Biểu đồ dự đoán vs thực tế\n",
    "plt.figure(figsize=(12, 8), clear=True)\n",
    "plt.scatter(predictions_pd[\"price\"], predictions_pd[\"prediction\"], alpha=0.5)\n",
    "plt.plot(\n",
    "    [predictions_pd[\"price\"].min(), predictions_pd[\"price\"].max()],\n",
    "    [predictions_pd[\"price\"].min(), predictions_pd[\"price\"].max()],\n",
    "    'r--'\n",
    ")\n",
    "plt.title('Giá trị dự đoán vs Thực tế', fontsize=15)\n",
    "plt.xlabel('Giá trị thực tế ($)', fontsize=12)\n",
    "plt.ylabel('Giá trị dự đoán ($)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Biểu đồ tầm quan trọng của đặc trưng\n",
    "plt.figure(figsize=(12, 8), clear=True)\n",
    "feature_importance = best_model.featureImportances.toArray()  # Chuyển sang mảng numpy\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "top_n = min(10, len(feature_names))\n",
    "\n",
    "plt.barh(range(top_n), feature_importance[indices][:top_n], align='center')\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in indices][:top_n])\n",
    "plt.xlabel('Tầm quan trọng', fontsize=12)\n",
    "plt.title('Top 10 đặc trưng quan trọng nhất', fontsize=15)\n",
    "plt.gca().invert_yaxis()  # Đảo ngược trục để đặc trưng quan trọng nhất ở trên\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
