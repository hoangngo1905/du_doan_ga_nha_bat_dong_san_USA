{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session\n",
    "print(\"Initializing Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Real Estate Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.jars.packages\", \"ml.dmlc:xgboost4j-spark_2.12:1.5.2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ====================== PH·∫¶N TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ======================\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "print(\"ƒêang ƒë·ªçc d·ªØ li·ªáu...\")\n",
    "df = spark.read.csv(\"realtor-data.zip.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Ki·ªÉm tra th√¥ng tin d·ªØ li·ªáu g·ªëc\n",
    "df.printSchema()\n",
    "# Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt\n",
    "df = df.drop(\"city\", \"zip_code\", \"prev_sold_date\")\n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng gi√° tr·ªã null\n",
    "null_counts = df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns])\n",
    "print(\"\\nS·ªë l∆∞·ª£ng gi√° tr·ªã null:\")\n",
    "null_counts.show()\n",
    "\n",
    "\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã null trong c·ªôt quan tr·ªçng\n",
    "df = df.filter(~col(\"price\").isNull())\n",
    "df = df.filter(~col(\"bed\").isNull())\n",
    "df = df.filter(~col(\"bath\").isNull())\n",
    "\n",
    "# X·ª≠ l√Ω c·ªôt status\n",
    "print(\"\\nGi√° tr·ªã trong c·ªôt status:\")\n",
    "df.groupBy(\"status\").count().show()\n",
    "df = df.drop(\"status\")\n",
    "\n",
    "# X·ª≠ l√Ω c·ªôt state\n",
    "print(\"\\nGi√° tr·ªã trong c·ªôt state:\")\n",
    "df.groupBy(\"state\").count().show()\n",
    "\n",
    "# Ch·ªâ gi·ªØ l·∫°i c√°c state c√≥ √≠t nh·∫•t 50 m·∫´u\n",
    "state_counts = df.groupBy(\"state\").count()\n",
    "valid_states = state_counts.filter(col(\"count\") >= 50).select(\"state\")\n",
    "df = df.join(valid_states, \"state\", \"inner\")\n",
    "\n",
    "# M√£ h√≥a state th√†nh s·ªë\n",
    "indexer = StringIndexer(inputCol=\"state\", outputCol=\"state_numeric\", handleInvalid=\"skip\")\n",
    "indexer_model = indexer.fit(df)\n",
    "df = indexer_model.transform(df)\n",
    "df = df.drop(\"state\")\n",
    "\n",
    "# T·∫°o mapping t·ª´ s·ªë ƒë·∫øn t√™n state\n",
    "state_labels = indexer_model.labels\n",
    "numeric_to_state = {i: state for i, state in enumerate(state_labels)}\n",
    "print(\"\\nMapping state_numeric -> state:\")\n",
    "for k, v in numeric_to_state.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ m√¥ t·∫£\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X·ª≠ l√Ω outlier cho c·ªôt price\n",
    "q95 = df.approxQuantile(\"price\", [0.95], 0.01)[0]\n",
    "q25 = df.approxQuantile(\"price\", [0.25], 0.01)[0]\n",
    "iqrMax = q95 + q25\n",
    "print(f\"\\nThreshold cho price: {iqrMax}\")\n",
    "percent_outliers = df.filter(col(\"price\") > 3150000.0).count() / df.count() * 100\n",
    "print(f\"T·ª∑ l·ªá outliers trong price: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"price\") <= 3150000.0)\n",
    "\n",
    "# X·ª≠ l√Ω outlier cho acre_lot\n",
    "percent_outliers = df.filter(col(\"acre_lot\") > 200).count() / df.count() * 100\n",
    "print(f\"\\nT·ª∑ l·ªá outliers trong acre_lot: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"acre_lot\") <= 200)\n",
    "\n",
    "# X·ª≠ l√Ω outlier cho house_size\n",
    "percent_outliers = df.filter(col(\"house_size\") >= 20000).count() / df.count() * 100\n",
    "print(f\"\\nT·ª∑ l·ªá outliers trong house_size: {percent_outliers:.2f}%\")\n",
    "df = df.filter(col(\"house_size\") < 20000)\n",
    "\n",
    "# Ki·ªÉm tra l·∫°i gi√° tr·ªã null sau khi x·ª≠ l√Ω outliers\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "print(\"\\nS·ªë l∆∞·ª£ng gi√° tr·ªã null sau khi x·ª≠ l√Ω outliers:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ m√¥ t·∫£\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id, when\n",
    "\n",
    "print(\"\\nƒêang ƒëi·ªÅn gi√° tr·ªã thi·∫øu cho acre_lot b·∫±ng XGBoost...\")\n",
    "\n",
    "# Ki·ªÉm tra n·∫øu c√≥ gi√° tr·ªã thi·∫øu\n",
    "if df.filter(col(\"acre_lot\").isNull()).count() > 0:\n",
    "\n",
    "    # Chuy·ªÉn d·ªØ li·ªáu c√≥ acre_lot sang Pandas\n",
    "    filled_pdf = filled_df.toPandas()\n",
    "    missing_pdf = missing_df.toPandas()\n",
    "\n",
    "    # Ki·ªÉm tra n·∫øu c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán\n",
    "    if not filled_pdf.empty and not missing_pdf.empty:\n",
    "        # Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "        X_train_fill = filled_pdf.drop(columns=[\"acre_lot\"])\n",
    "        y_train_fill = filled_pdf[\"acre_lot\"]\n",
    "\n",
    "        X_test_fill = missing_pdf.drop(columns=[\"acre_lot\"])\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost\n",
    "        model_fill = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8\n",
    "        )\n",
    "        model_fill.fit(X_train_fill, y_train_fill)\n",
    "\n",
    "        # D·ª± ƒëo√°n gi√° tr·ªã thi·∫øu\n",
    "        preds = model_fill.predict(X_test_fill)\n",
    "\n",
    "        # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ DataFrame PySpark\n",
    "        preds_df = pd.DataFrame({\"acre_lot\": preds})\n",
    "        preds_spark = spark.createDataFrame(preds_df)\n",
    "\n",
    "        # Th√™m ID ƒë·ªÉ k·∫øt h·ª£p l·∫°i d·ªØ li·ªáu\n",
    "        df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        missing_df = missing_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds_spark = preds_spark.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "        # K·∫øt h·ª£p gi√° tr·ªã d·ª± ƒëo√°n v√†o t·∫≠p d·ªØ li·ªáu g·ªëc\n",
    "        df = df.join(preds_spark, \"id\", \"left_outer\").drop(\"id\")\n",
    "        df = df.withColumn(\"acre_lot\", when(col(\"acre_lot\").isNull(), col(\"prediction\")).otherwise(col(\"acre_lot\"))).drop(\"prediction\")\n",
    "\n",
    "        print(f\"ƒê√£ ƒëi·ªÅn {len(preds)} gi√° tr·ªã thi·∫øu cho acre_lot b·∫±ng XGBoost!\")\n",
    "\n",
    "    else:\n",
    "        print(\"Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán XGBoost. B·ªè qua vi·ªác ƒëi·ªÅn gi√° tr·ªã thi·∫øu.\")\n",
    "\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong acre_lot. Kh√¥ng c·∫ßn ƒëi·ªÅn.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id, when\n",
    "\n",
    "print(\"\\nƒêang ƒëi·ªÅn gi√° tr·ªã thi·∫øu cho house_size b·∫±ng XGBoost...\")\n",
    "\n",
    "# Ki·ªÉm tra n·∫øu c√≥ gi√° tr·ªã thi·∫øu\n",
    "if df.filter(col(\"house_size\").isNull()).count() > 0:\n",
    "\n",
    "    # Chuy·ªÉn d·ªØ li·ªáu PySpark sang Pandas\n",
    "    filled_pdf = df.filter(col(\"house_size\").isNotNull()).toPandas()\n",
    "    missing_pdf = df.filter(col(\"house_size\").isNull()).toPandas()\n",
    "\n",
    "    # Ki·ªÉm tra n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán\n",
    "    if not filled_pdf.empty and not missing_pdf.empty:\n",
    "        # Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "        X_train_fill = filled_pdf.drop(columns=[\"house_size\"])\n",
    "        y_train_fill = filled_pdf[\"house_size\"]\n",
    "\n",
    "        X_test_fill = missing_pdf.drop(columns=[\"house_size\"])\n",
    "\n",
    "        # X·ª≠ l√Ω gi√° tr·ªã NaN b·∫±ng gi√° tr·ªã trung b√¨nh\n",
    "        X_train_fill = X_train_fill.fillna(X_train_fill.mean())\n",
    "        X_test_fill = X_test_fill.fillna(X_train_fill.mean())\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost\n",
    "        model_fill = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method='hist'\n",
    "        )\n",
    "        model_fill.fit(X_train_fill, y_train_fill)\n",
    "\n",
    "        # D·ª± ƒëo√°n gi√° tr·ªã thi·∫øu\n",
    "        preds = model_fill.predict(X_test_fill)\n",
    "\n",
    "        # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ DataFrame PySpark\n",
    "        preds_df = pd.DataFrame({\"house_size\": preds})\n",
    "        preds_spark = spark.createDataFrame(preds_df)\n",
    "\n",
    "        # Th√™m ID ƒë·ªÉ k·∫øt h·ª£p l·∫°i d·ªØ li·ªáu\n",
    "        df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "        missing_df = df.filter(col(\"house_size\").isNull()).withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds_spark = preds_spark.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "        # K·∫øt h·ª£p gi√° tr·ªã d·ª± ƒëo√°n v√†o t·∫≠p d·ªØ li·ªáu g·ªëc\n",
    "        df = df.join(preds_spark, \"id\", \"left_outer\").drop(\"id\")\n",
    "        df = df.withColumn(\"house_size\", when(col(\"house_size\").isNull(), col(\"prediction\")).otherwise(col(\"house_size\"))).drop(\"prediction\")\n",
    "\n",
    "        print(f\"ƒê√£ ƒëi·ªÅn {len(preds)} gi√° tr·ªã thi·∫øu cho house_size b·∫±ng XGBoost!\")\n",
    "\n",
    "    else:\n",
    "        print(\"Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán XGBoost. B·ªè qua vi·ªác ƒëi·ªÅn gi√° tr·ªã thi·∫øu.\")\n",
    "\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong house_size. Kh√¥ng c·∫ßn ƒëi·ªÅn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "print(\"ƒêang t·∫°o c√°c ƒë·∫∑c tr∆∞ng m·ªõi...\")\n",
    "\n",
    "# Tr√°nh chia cho 0 b·∫±ng c√°ch s·ª≠ d·ª•ng F.when\n",
    "df = df.withColumn(\"bed_bath_ratio\", col(\"bed\") / when(col(\"bath\") == 0, 1).otherwise(col(\"bath\")))\n",
    "df = df.withColumn(\"total_rooms\", col(\"bed\") + col(\"bath\"))\n",
    "df = df.withColumn(\"room_density\", col(\"total_rooms\") / when(col(\"house_size\") == 0, 1).otherwise(col(\"house_size\")))\n",
    "df = df.withColumn(\"house_size_per_bed\", col(\"house_size\") / when(col(\"bed\") == 0, 1).otherwise(col(\"bed\")))\n",
    "df = df.withColumn(\"house_size_per_bath\", col(\"house_size\") / when(col(\"bath\") == 0, 1).otherwise(col(\"bath\")))\n",
    "df = df.withColumn(\"lot_to_house_ratio\", col(\"acre_lot\") / when(col(\"house_size\") == 0, 1).otherwise(col(\"house_size\")))\n",
    "df = df.withColumn(\"size_by_state\", col(\"house_size\") * col(\"state_numeric\"))\n",
    "df = df.withColumn(\"rooms_by_state\", col(\"total_rooms\") * col(\"state_numeric\"))\n",
    "\n",
    "# ƒêi·ªÅn gi√° tr·ªã thi·∫øu cho c√°c c·ªôt s·ªë (tr√°nh l·ªói)\n",
    "numeric_columns = [c for c, t in df.dtypes if t in ('int', 'double')]\n",
    "df = df.fillna(0, subset=numeric_columns)\n",
    "\n",
    "print(\"Ho√†n t·∫•t t·∫°o ƒë·∫∑c tr∆∞ng m·ªõi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nChu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh...\")\n",
    "\n",
    "# X√°c ƒë·ªãnh c·ªôt ƒë·∫∑c tr∆∞ng (b·ªè c·ªôt price)\n",
    "feature_cols = [c for c in df.columns if c != \"price\"]\n",
    "\n",
    "# D√πng VectorAssembler ƒë·ªÉ g·ªôp t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng v√†o m·ªôt c·ªôt duy nh·∫•t\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"price\")\n",
    "\n",
    "# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: {train_df.count()} m·∫´u, t·∫≠p ki·ªÉm tra: {test_df.count()} m·∫´u\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nB·∫Øt ƒë·∫ßu t√¨m ki·∫øm si√™u tham s·ªë t·ªëi ∆∞u...\")\n",
    "\n",
    "# S·ª≠ d·ª•ng GBTRegressor v·ªõi c√°c tham s·ªë ƒë∆∞·ª£c t·ªëi ∆∞u\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"price\",\n",
    "    maxBins=32,        # TƒÉng maxBins ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t\n",
    "    maxIter=100,       # S·ªë l·∫ßn l·∫∑p\n",
    "    stepSize=0.1,      # Learning rate\n",
    "    maxDepth=5,        # ƒê·ªô s√¢u c√¢y\n",
    "    subsamplingRate=0.8  # Subsampling rate\n",
    ")\n",
    "\n",
    "# Gi·∫£m tham s·ªë trong l∆∞·ªõi t√¨m ki·∫øm ƒë·ªÉ tr√°nh timeout\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# S·ª≠ d·ª•ng TrainValidationSplit v·ªõi t·ª∑ l·ªá train cao h∆°n\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.9,  # 90% hu·∫•n luy·ªán, 10% ki·ªÉm ƒë·ªãnh\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "print(\"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh, vui l√≤ng ƒë·ª£i...\")\n",
    "model = tvs.fit(train_df)\n",
    "\n",
    "# L·∫•y m√¥ h√¨nh t·ªët nh·∫•t\n",
    "best_model = model.bestModel\n",
    "print(\"\\nƒê√£ t√¨m th·∫•y m√¥ h√¨nh t·ªët nh·∫•t!\")\n",
    "print(f\"Best maxDepth: {best_model.getMaxDepth()}\")\n",
    "print(f\"Best maxIter: {best_model.getMaxIter()}\")\n",
    "print(f\"Best stepSize: {best_model.getStepSize()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"\\nƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p ki·ªÉm tra...\")\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "predictions = best_model.transform(test_df)\n",
    "\n",
    "# Kh·ªüi t·∫°o b·ªô ƒë√°nh gi√°\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(\"\\nüîç ƒê√°nh gi√° m√¥ h√¨nh tr√™n d·ªØ li·ªáu th·ª±c t·∫ø:\")\n",
    "print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"‚úÖ RMSE: ${rmse:.2f}\")\n",
    "print(f\"‚úÖ MAE: ${mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ƒêang v·∫Ω bi·ªÉu ƒë·ªì...\")\n",
    "\n",
    "# Chuy·ªÉn d·ªØ li·ªáu PySpark v·ªÅ Pandas\n",
    "predictions_pd = predictions.select(\"price\", \"prediction\").toPandas()\n",
    "\n",
    "# 1. Bi·ªÉu ƒë·ªì d·ª± ƒëo√°n vs th·ª±c t·∫ø\n",
    "plt.figure(figsize=(12, 8), clear=True)\n",
    "plt.scatter(predictions_pd[\"price\"], predictions_pd[\"prediction\"], alpha=0.5)\n",
    "plt.plot(\n",
    "    [predictions_pd[\"price\"].min(), predictions_pd[\"price\"].max()],\n",
    "    [predictions_pd[\"price\"].min(), predictions_pd[\"price\"].max()],\n",
    "    'r--'\n",
    ")\n",
    "plt.title('Gi√° tr·ªã d·ª± ƒëo√°n vs Th·ª±c t·∫ø', fontsize=15)\n",
    "plt.xlabel('Gi√° tr·ªã th·ª±c t·∫ø ($)', fontsize=12)\n",
    "plt.ylabel('Gi√° tr·ªã d·ª± ƒëo√°n ($)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bi·ªÉu ƒë·ªì t·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng\n",
    "plt.figure(figsize=(12, 8), clear=True)\n",
    "feature_importance = best_model.featureImportances.toArray()  # Chuy·ªÉn sang m·∫£ng numpy\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "top_n = min(10, len(feature_names))\n",
    "\n",
    "plt.barh(range(top_n), feature_importance[indices][:top_n], align='center')\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in indices][:top_n])\n",
    "plt.xlabel('T·∫ßm quan tr·ªçng', fontsize=12)\n",
    "plt.title('Top 10 ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t', fontsize=15)\n",
    "plt.gca().invert_yaxis()  # ƒê·∫£o ng∆∞·ª£c tr·ª•c ƒë·ªÉ ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t ·ªü tr√™n\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
